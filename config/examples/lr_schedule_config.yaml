# Example configuration for learning rate scheduling
# This file can be used to configure different learning rate schedules
# including warmup and various decay strategies.

training:
  # Basic training parameters
  batch_size: 32
  epochs: 100
  learning_rate: 0.001
  optimizer: "adam"
  loss: "categorical_crossentropy"
  
  # Learning rate scheduling configuration
  lr_schedule:
    enabled: true
    # Type can be: warmup_cosine, warmup_exponential, warmup_step, one_cycle
    type: "warmup_cosine"
    # Number of warmup epochs
    warmup_epochs: 5
    # Minimum learning rate
    min_lr: 1.0e-6

  # Alternative: One-Cycle learning rate policy
  # Uncomment and modify these settings to use one-cycle policy
  # lr_schedule:
  #   enabled: true
  #   type: "one_cycle"
  #   # Maximum learning rate
  #   max_lr: 0.01
  #   # Factor to determine initial learning rate (max_lr / div_factor)
  #   div_factor: 25.0
  #   # Percentage of cycle spent increasing learning rate
  #   pct_start: 0.3
  #   # Minimum learning rate
  #   min_lr: 1.0e-6

  # Model quantization settings
  quantization:
    enabled: false
    # Quantization type: "post_training" or "during_training"
    type: "post_training"
    # Quantization format: "int8", "float16", etc.
    format: "int8"
    # Whether to optimize for inference
    optimize_for_inference: true
    # Whether to measure performance after quantization
    measure_performance: true

  # Model pruning settings
  pruning:
    enabled: false
    # Pruning type: "magnitude", "structured", etc.
    type: "magnitude"
    # Target sparsity (percentage of weights to prune)
    target_sparsity: 0.5
    # Whether to perform pruning during training
    during_training: true
    # Pruning schedule: "constant" or "polynomial"
    schedule: "polynomial"
    # Start step for pruning
    start_step: 0
    # End step for pruning
    end_step: 100
    # Pruning frequency (every N steps)
    frequency: 10